<!DOCTYPE html>

<html>
    <head>
        <meta charset="utf-8">
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0"> 
        
        <meta name="description" content="Audio-Visual Demos">
        <meta name="author" content="Ke Tan">
        
	    <title>Audio-Visual Demos</title>
        
        <!-- Bootflat CSS -->
        <link rel="stylesheet" href="css/bootflat.css">

        <!-- Bootstrap Core CSS -->
        <link href="css/bootstrap.min.css" rel="stylesheet">

        <!-- Custom CSS -->
        <link href="css/index.css" rel="stylesheet">

        <!-- Custom Fonts -->
        <link href="font-awesome-4.7.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
        <link href="http://fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic,700italic" rel="stylesheet" type="text/css">
    </head>

    
    <body>
        <div id="container">
            <div id="paper_title"> <h1>Audio-Visual Speech Separation and Dereverberation <br> with a Two-Stage Multimodal Network</h1> </div>
            <div id="paper_authors"> <b>Authors:</b> Ke Tan, Yong Xu, Shi-Xiong Zhang, Meng Yu, and Dong Yu </div>
            <div id="paper_abstract"> <b>Abstract:</b> Background noise, interfering speech and room reverberation frequently distort target speech in real listening environments. In this study, we address joint speech separation and dereverberation, which aims to separate target speech from background noise, interfering speech and room reverberation. In order to tackle this fundamentally difficult problem, we propose a novel multimodal network that exploits both audio and visual signals. The proposed network architecture adopts a two-stage strategy, where a separation module is employed to attenuate background noise and interfering speech in the first stage and a dereverberation module to suppress room reverberation in the second stage. The two modules are first trained separately, and then integrated for joint training, which is based on a new multi-objective loss function. Our experimental results show that the proposed multimodal network yields consistently better objective intelligibility and perceptual quality than several one-stage and two-stage baselines. We find that our network achieves a 21.10% improvement in ESTOI and a 0.79 improvement in PESQ over the unprocessed mixtures. Moreover, our network architecture does not require the knowledge of the number of speakers.</div>
            
            <div id="description">
                <h4>A Two-Stage Multimodel Network for Joint Separation and Dereverberation in Far-Field Scenarios</h4>
                <p>Our multimodel network adopts a two-stage strategy, as shown in Fig. 1. A dilated CNN based separation module, which takes both audio and visual inputs, is employed to separate reverberant target speech from interfering speech and background noise. The output of the separation module is subsequently passed through a BLSTM based dereverberation module. The two modules are first trained separately and then trained jointly to optimize a new multi-objective loss function, which combines a time domain loss and a time-frequency domain loss.</p>
                <p>For full details, see <a href="https://arxiv.org/abs/1909.07352" target="_blank">our paper</a>.</p>
                <a><img src="img/architecture.png" width="100%" id="architecture"/></a>
                <div id="architecture_caption"><b>Fig. 1. Our proposed two-stage multimodal network architecture for joint separation and dereverberation.</b></div>          
            </div>
            
            <div id="demos">
                <h4>Demos</h4>
                <div id="demo1" style="float:left;">
                    <video id="video1" width="380" controls>
                        <source src="video/reverb_mix.mp4" type="video/mp4" />
                    </video>
                    <div id="video1_des"><b>Unprocessed mixture <br> (the camera only captures the side face of the target speaker)</b></div>
                </div>    
                <div id="demo2" style="float:right;">
                    <video id="video2" width="380" controls>
                        <source src="video/reverb_enh.mp4" type="video/mp4" />
                    </video>
                    <div id="video2_des"><b>Processed speech</b></div>
                </div>
            </div> 
       
        </div>
    
    </body>
</html>
